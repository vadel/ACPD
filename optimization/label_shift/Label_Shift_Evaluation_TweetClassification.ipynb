{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import pulp\n",
    "from scipy.stats import entropy\n",
    "import scipy as sp\n",
    "\n",
    "import warnings\n",
    "\n",
    "from acpd_utils import check_results\n",
    "from acpd_utils import validate_T_matrix\n",
    "from acpd_methods import generate_T_method_3\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns True if all the values in 'x' are integers (even if they have float format) \n",
    "def assert_int_array(x):\n",
    "    return np.all(np.abs(np.round(x)-x)<1e-8)\n",
    "\n",
    "\n",
    "#Given an array of probabilities 'probs' and a number of samples, returns the absolute frequencies \n",
    "#corresponding to probs*num_samples. \n",
    "#2 possible control levels: \n",
    "#   expect_int_freqs=True:  stops if the frequency vector is not an integer array\n",
    "#   expect_int_freqs=False: the computed frequencies are rounded in order to be integers \n",
    "#                           (and a warning message is raised)\n",
    "def probs2freqs(probs, num_samples, expect_int_freqs):\n",
    "    #Multiply the probabilities by the number of samples\n",
    "    probs_samples = np.copy(probs*num_samples)\n",
    "    \n",
    "    #Check if the frequencies are integers (if not, either raise an error or round the values):\n",
    "    if not assert_int_array(probs_samples):\n",
    "        if expect_int_freqs:\n",
    "            sys.exit(\"probs*num_samples is not an Int array (freqs)\") #Return an error\n",
    "        else:\n",
    "            warnings.warn(\"Freqs=probs*num_samples is not an Int array\")\n",
    "            warnings.warn(\"Rounding will be applied to the frequencies\")\n",
    "            probs_samples = np.round(probs_samples) #Round the frequencies\n",
    "            \n",
    "    assert assert_int_array(probs_samples) #Final sanity check\n",
    "    \n",
    "    #Return the vector with the absolute frequencies\n",
    "    return probs_samples\n",
    "\n",
    "\n",
    "#Given an array of frequencies 'freqs' (obtained by multiplying a vector of probabilities with a number of samples),\n",
    "#correct the absolute frequencies (if required) in order to ensure that the sum matches that number of samples.\n",
    "def correct_rounded_freqs(freqs, num_samples):\n",
    "    out_freqs = np.copy(freqs)\n",
    "    #Compute difference between total frequencies and the expected number of samples\n",
    "    tmp_dif = int(np.abs(np.sum(out_freqs) - num_samples))\n",
    "    #Fix the frequencies:\n",
    "    if np.sum(out_freqs)>num_samples:  \n",
    "        out_freqs[np.argmax(out_freqs)] -= tmp_dif #decrease from the element with max. frequency\n",
    "    else:\n",
    "        out_freqs[np.argmin(out_freqs)] += tmp_dif #increase from the element with min. frequency\n",
    "    #Sanity checks\n",
    "    assert np.sum(out_freqs)==num_samples, \"Frequencies still wrong after correction: %d\" % np.sum(out_freqs)\n",
    "    assert np.all(out_freqs>=0) and np.all(out_freqs<=num_samples), \"Out of bound values in frequencies\"\n",
    "    assert assert_int_array(out_freqs)\n",
    "    \n",
    "    return out_freqs\n",
    "\n",
    "\n",
    "\n",
    "#TWO SAMPLE TESTS\n",
    "#Evaluates a specified two sample statistical test (categorical variables) for the label-shift detector, \n",
    "#and returns the p-value.\n",
    "# @input labels:       labels/categories of the problem\n",
    "# @input num_samples:  number of elements in the 'samples'\n",
    "# @input py_exp:       'expected' probability distribution\n",
    "# @input py_obs:       'observed' probability distribution\n",
    "# @input test_method:  statistical test to be emploted. Supported option: \"chisq_cont\" (Chi-Square test)\n",
    "# @output:             p-value of the statistical test\n",
    "def evaluate_test(labels, num_samples, py_exp, py_obs, test_method):\n",
    "    #Convert the probabilities into absolute frequencies\n",
    "    py_exp_samples = probs2freqs(py_exp, num_samples, expect_int_freqs=True)\n",
    "    py_obs_samples = probs2freqs(py_obs, num_samples, expect_int_freqs=False)\n",
    "    \n",
    "    #Convert to integer arrays\n",
    "    py_exp_samples = np.array(py_exp_samples, dtype=np.int64)\n",
    "    py_obs_samples = np.array(py_obs_samples, dtype=np.int64)\n",
    "    \n",
    "    #Sanity checks\n",
    "    if np.sum(py_exp_samples)!=num_samples: \n",
    "        sys.exit(\"Total frequency of py_exp (%d) does not match num_samples (%d)\" % (np.sum(py_exp_samples), num_samples))\n",
    "    if np.sum(py_obs_samples)!=num_samples: \n",
    "        warnings.warn(\"Total frequency of py_obs (%d) does not match num_samples (%d)\" % (np.sum(py_obs_samples), num_samples))\n",
    "        warnings.warn(\"Total frequency of py_obs will be corrected.\")\n",
    "        py_obs_samples = correct_rounded_freqs(py_obs_samples, num_samples) #Correct the frequencies\n",
    "    assert np.sum(py_exp_samples)==np.sum(py_obs_samples), \"The total frequency of the two vectors is not the same\"\n",
    "    assert np.sum(py_obs_samples)==num_samples, \"Frequencies do not match expected\"\n",
    "    assert len(labels)==len(py_exp) and len(labels)==len(py_obs), \"Wrong number of categories\"\n",
    "    \n",
    "    #Run the statistical test    \n",
    "    if test_method==\"chisq_cont\":\n",
    "        statistic, pvalue, cont_dof, cont_expec = sp.stats.chi2_contingency(np.array([py_exp_samples,py_obs_samples]))\n",
    "    else:\n",
    "        sys.exit(\"Statistical test '%s' not supported!\" % test_method)\n",
    "        \n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Auxiliary function to adjust a target distribution so that it is not significantly different from a source\n",
    "#distribution, according to a statistical test (specified by the 'test_method' argument).\n",
    "#Given a reference distribution 'p_y_init' and a probability distribution 'p_y_target_signif',\n",
    "#finds and returns another distribution 'p_y_target',\n",
    "#  p_y_target = (1-\\alpha)*p_y_init + (\\alpha)*p_y_target_signif\n",
    "#by finding which is the maximum value of \\alpha so that there are not significant differences between\n",
    "#p_y_target and p_y_init, according to a selected statistical test (specified by the 'test_method' argument).\n",
    "def make_py_target_not_significant(p_y_init, p_y_target_signif,\n",
    "                                   py_test_tol, py_test_width,\n",
    "                                   eval_n_samp, labels, test_method):\n",
    "    \n",
    "    iters = 0 #number of binary-search iterations\n",
    "    \n",
    "    #Evaluate if the target distribution is significantly different to p_y_init\n",
    "    p_y_target = np.copy(p_y_target_signif)\n",
    "    pvalue = evaluate_test(labels, eval_n_samp, py_exp=p_y_init, py_obs=p_y_target, test_method=test_method)\n",
    "    \n",
    "    if pvalue>=(py_test_tol+py_test_width):\n",
    "        py_alpha_ratio = 1 #Trade-off ratio between the initial distribution and the target distribution\n",
    "        print(\"The provided distribution is not detected as significantly different from P(Y)\")\n",
    "        warnings.warn(\"The provided distribution is not detected as significantly different from P(Y)\")\n",
    "    else:\n",
    "        py_alpha_ratio = -1 #Trade-off ratio (initialized as negative value)\n",
    "        \n",
    "        #Binary search to find the maximum \"alpha\" parameter for which the target distribution is not detected\n",
    "        #as a significant change by the selected statistical test.\n",
    "        l_alpha = 0.0\n",
    "        r_alpha = 1.0\n",
    "        while r_alpha-l_alpha > 1e-8:\n",
    "            mid_alpha = (l_alpha+r_alpha)/2.0\n",
    "            mid_p_y   = (1-mid_alpha)*p_y_init + mid_alpha*p_y_target_signif\n",
    "\n",
    "            #Sanity checks\n",
    "            assert np.abs(np.sum(mid_p_y)-1.0)<1e-8, \"mid_p_y  is not a prob. distr.\"\n",
    "            assert np.all(mid_p_y>0.0) and np.all(mid_p_y<1.0), \"mid_p_y out  of [0,1]\"\n",
    "            \n",
    "            #Evaluate the test\n",
    "            pvalue = evaluate_test(labels, eval_n_samp, py_exp=p_y_init, py_obs=mid_p_y, test_method=test_method)\n",
    "            #Adjust the search\n",
    "            if pvalue <= (py_test_tol+py_test_width):\n",
    "                r_alpha = mid_alpha #significant --> reduce alpha (more similar to p_y_init) \n",
    "            else:\n",
    "                l_alpha = mid_alpha #non-significant --> increase alpha (more similar to p_y_target_signif)\n",
    "                p_y_target = np.copy(mid_p_y) #set the current distribution as the target \n",
    "                assert py_alpha_ratio<mid_alpha  #Sanity check\n",
    "                py_alpha_ratio = np.copy(mid_alpha) #update the ratio\n",
    "            iters+=1\n",
    "        \n",
    "        assert py_alpha_ratio>0.0 and py_alpha_ratio<1.0\n",
    "    \n",
    "    \n",
    "    #Compute the final p-value\n",
    "    pvalue = evaluate_test(labels, eval_n_samp, py_exp=p_y_init, py_obs=p_y_target, test_method=test_method)\n",
    "    #Print some details\n",
    "    print(\">> Iterations required:\", iters)\n",
    "    print(\">> Balance-Ratio\", py_alpha_ratio)\n",
    "    print(\">> Final p-value:\", pvalue)\n",
    "        \n",
    "    #Sanity checks\n",
    "    assert np.abs(np.sum(p_y_target)-1.0)<1e-8, \"p_y_target_weighted  is not a prob. distr.\"\n",
    "    assert np.all(p_y_target>0.0) and np.all(p_y_target<1.0), \"p_y_target_weighted not in [0,1]\"\n",
    "    assert pvalue>=(py_test_tol+py_test_width), \"Final distribution is significantly different\"\n",
    "    \n",
    "    return p_y_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Root path of the project\n",
    "root_path = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select the dataset\n",
    "sel_dataset = \"emotion\" #Tweet Emotion Classification \n",
    "print(\"Main setup: \\t \" + sel_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Auxiliary function to get the model \"ID\" (abbreviated identifier) given the \"HuggingFaces tag\"\n",
    "#The ID is only used for an easier saving/loading process\n",
    "def text_model_tag2id(main_setup, model_tag, base_path):\n",
    "    load_filename = base_path + \"models/\" + \"%s_tag2id_dict.npy\" % main_setup  # Filename\n",
    "    model_tag2id  = np.load(load_filename, allow_pickle=True)[()]  # Load the dictionary which maps tags to IDs\n",
    "    assert model_tag in model_tag2id, \"Tag not found in the dictionary: %s\" % model_tag #Sanity check\n",
    "    return model_tag2id[model_tag] #Return the ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select a target model (from bhadresh-savani - HuggingFace repository)\n",
    "model_tag = \"bert-base-uncased-emotion\"  #** The model used for the experiments\n",
    "model_id  = text_model_tag2id(sel_dataset, model_tag, root_path)  # model ID (abbreviated identifier)\n",
    "print(\"Model tag: %s (id: %s)\" % (model_tag, model_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the problem labels\n",
    "labels_file_path = root_path + \"datasets/%s_dataset_train/%s_labels.txt\" % (sel_dataset, sel_dataset)\n",
    "labels = open(labels_file_path, 'r').readlines()\n",
    "labels = [s.rstrip() for s in labels]\n",
    "print(\"Labels\", labels)\n",
    "\n",
    "num_classes = len(labels) #Number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select an adversarial attack\n",
    "sel_attack = \"genetic\"  #Genetic algorithm-based word substitution\n",
    "print(\"Sel attack: %s\" % sel_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select a source distribution:\n",
    "p_y_init_idx = 0   #Options: [0,1,2]\n",
    "\n",
    "#In the evaluation we considered three different source distributions P(Y):\n",
    "if   p_y_init_idx==0:  p_y_init = np.array([0.17, 0.17, 0.17, 0.17, 0.17, 0.15]) #Roughly uniform\n",
    "elif p_y_init_idx==1:  p_y_init = np.array([0.15, 0.25, 0.15, 0.15, 0.15, 0.15]) #Tweak-2\n",
    "elif p_y_init_idx==2:  p_y_init = np.array([0.15, 0.15, 0.15, 0.25, 0.15, 0.15]) #Tweak-4\n",
    "else: sys.exit(\"Invalid identifier for the source distribution: %d\" % p_y_init_idx)\n",
    "    \n",
    "#Sanity checks\n",
    "assert np.abs(np.sum(p_y_init)-1.0)<=1e-8, np.sum(p_y_init)\n",
    "assert np.all(p_y_init>=0.0) and np.all(p_y_init<=1.0)\n",
    "\n",
    "print(\"Source distribution (index %d):\" % p_y_init_idx)\n",
    "print(p_y_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Statistical test for the label-shift detector (BBSD)\n",
    "selected_stat_test = \"chisq_cont\"  #Chi-Square test\n",
    "print(\"Selected statistical test: %s\" % selected_stat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select the tolerance for the statistical test of the label-shift detector\n",
    "py_test_tol   = 0.00001 \n",
    "py_test_width = 0.0001  #Width parameter for the tolerance (to be used during the sampling procedure)\n",
    "\n",
    "print(\"Stat. test tolerance: %s + width: %s\" % (str(py_test_tol), str(py_test_width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the path in which the results will be saved (if save_path is None, the results will not be saved)\n",
    "#save_path = None\n",
    "save_path = root_path + \"optimization/label_shift/%s/%s/\" % (sel_dataset, sel_attack)\n",
    "\n",
    "if not (save_path is None):\n",
    "    print(\"Save path: %s\" % save_path)\n",
    "else:\n",
    "    print(\"Results will not be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data to be used in the experimental evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the sampling considered for the experiments (selected dataset indices and ground-truth labels)\n",
    "experiment_folder = root_path + \"adv_attacks/text/experiments/\"\n",
    "full_filenames    = np.load(experiment_folder + \"%s_%s_full_sampling.npy\"%(sel_dataset,model_id)) #indices\n",
    "full_ground_truth = np.load(experiment_folder + \"%s_%s_full_labels.npy\"%(sel_dataset,model_id))   #labels\n",
    "\n",
    "\n",
    "#Load the preprocessed results corresponding to the selected adversarial attacks\n",
    "results_root = root_path + \"adv_attacks/text/analysis/targeted/%s/%s/\" % (sel_dataset, model_id)\n",
    "appendix_param_npy = \"%s_targ.npy\" % sel_attack\n",
    "\n",
    "\n",
    "#The following matrices contain information about targeted attacks applied to a set of inputs (row-wise).\n",
    "#This information is already precomputed for the sake of a more efficient evaluation process.\n",
    "#The i-th row corresponds to an input sample, and the columns represent the (target) classes.\n",
    "#Thus, for each row \"i\" and column \"j\", the following matrices represent:\n",
    "# 1) whether it was possible to reach target class y_j from the i-th input (=1) or not (=0).\n",
    "full_reachability       = np.load(results_root + \"reachability_\" + appendix_param_npy)\n",
    "# 2) the amount of distortion required to reach the class y_j from the i-th input.\n",
    "full_reachability_norms = np.load(results_root + \"dist_vec_\" + appendix_param_npy)\n",
    "# 3) the number of steps/\"model queries\" required to reach the class y_j from the i-th input.\n",
    "full_reachability_steps = np.load(results_root + \"num_steps_vec_\" + appendix_param_npy)\n",
    "# 4) the time required to generate the corresponding attack.\n",
    "full_reachability_time  = np.load(results_root + \"secs_vec_\" + appendix_param_npy)\n",
    "\n",
    "\n",
    "#Now we \"nullify\" those attacks that surpass any of the thresholds, namely, the maximum\n",
    "#number of iterations or the maximum distortion allowed.\n",
    "max_dist_thr = 0.25    #Distortion threshold (\\epsilon)\n",
    "max_iter_thr = np.inf  #Number of iterations (no limit is set)\n",
    "\n",
    "#Nullify the \"reach\" of those attacks that exceed the thresholds\n",
    "for i in range(full_reachability.shape[0]):\n",
    "    for j in range(num_classes):\n",
    "        #The norm surpasses the threshold:\n",
    "        if full_reachability_norms[i,j] > max_dist_thr:\n",
    "            full_reachability[i,j] = 0\n",
    "        #The number of steps surpasses the threshold\n",
    "        if full_reachability_steps[i,j] > max_iter_thr:\n",
    "            full_reachability[i,j] = 0\n",
    "\n",
    "\n",
    "#Sanity checks:\n",
    "assert full_reachability.shape[0] == len(full_filenames)\n",
    "assert full_reachability.shape[1] == num_classes\n",
    "#Ensure that the ground-truth class can always be reached\n",
    "for i in range(full_reachability.shape[0]):\n",
    "    assert full_reachability[i,full_ground_truth[i]]==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of samples per class in the loaded set\n",
    "N_per_class = [np.sum(full_ground_truth==l) for l in range(len(labels))]\n",
    "N_per_class = np.array(N_per_class, dtype=np.int16)\n",
    "\n",
    "\n",
    "#Total number of samples to be considered for the experimental evaluation\n",
    "N_samp = 1000\n",
    "#Number of classes per class for the experimental evaluation. In both cases, the proportions are determined\n",
    "#by the source probability distribution.\n",
    "N_per_class_train = np.array(p_y_init*N_samp, dtype=np.int16) #to generate the transition matrices\n",
    "N_per_class_valid = np.array(p_y_init*N_samp, dtype=np.int16) #to evaluate their effectiveness on different inputs\n",
    "\n",
    "#Sanity checks\n",
    "assert np.sum(N_per_class_train)==N_samp\n",
    "assert np.sum(N_per_class_valid)==N_samp\n",
    "for l in range(len(labels)):\n",
    "    assert (N_per_class_train[l]+N_per_class_valid[l])<=N_per_class[l]\n",
    "\n",
    "\n",
    "\n",
    "#Create a hold-out partition for the experiments \n",
    "np.random.seed(10)\n",
    "\n",
    "#Auxiliary mask to assess if a sample has been already selected\n",
    "samp_visited = [0 for i in range(len(full_filenames))]\n",
    "\n",
    "#Initialize emtpy lists to store the indices of each partition\n",
    "train_partition = []\n",
    "valid_partition = []\n",
    "\n",
    "for partition in [\"train\", \"valid\"]:\n",
    "    for l in range(len(labels)):\n",
    "        cont = 0\n",
    "        #Get the expected number of samples for the current label\n",
    "        if partition==\"train\":\n",
    "            expected_cont = np.copy(N_per_class_train[l])\n",
    "        else:\n",
    "            expected_cont = np.copy(N_per_class_valid[l])\n",
    "        \n",
    "        #Sample:\n",
    "        for i in range(len(full_filenames)):\n",
    "            #Check if the current sample is eligible\n",
    "            if samp_visited[i]==0 and full_ground_truth[i]==l:\n",
    "                samp_visited[i]=1 #mark it as selected (unavailable for future usage)\n",
    "                cont+=1           #increase the counter\n",
    "                #Add the index to the corresponding partition\n",
    "                if partition==\"train\":\n",
    "                    train_partition.append(i)\n",
    "                else:\n",
    "                    valid_partition.append(i)\n",
    "                    \n",
    "            #If we already achieved the desired number of samples for this category, exit\n",
    "            if cont==expected_cont: \n",
    "                break\n",
    "        #Sanity check\n",
    "        assert cont==expected_cont, \"%s, %d %d\"%(partition, cont, expected_cont)\n",
    "\n",
    "#Convert the lists to numpy arrays\n",
    "train_partition = np.array(train_partition, dtype=np.int16)\n",
    "valid_partition = np.array(valid_partition, dtype=np.int16)\n",
    "\n",
    "#Randomly shuffle the order of the indices\n",
    "np.random.shuffle(train_partition)\n",
    "np.random.shuffle(valid_partition)\n",
    "\n",
    "#Sanity checks\n",
    "assert np.sum(N_per_class_train)==len(train_partition)\n",
    "assert np.sum(N_per_class_valid)==len(valid_partition)\n",
    "assert np.sum(samp_visited)==(np.sum(N_per_class_train)+np.sum(N_per_class_valid)), \"Wrong sampling size\"\n",
    "for l in range(len(labels)):\n",
    "    assert np.sum(full_ground_truth[train_partition]==l) == N_per_class_train[l]\n",
    "    assert np.sum(full_ground_truth[valid_partition]==l) == N_per_class_valid[l]\n",
    "\n",
    "    \n",
    "#Split the attack information according to the partitions\n",
    "#Reach of the attacks:\n",
    "reachability_train = np.copy(full_reachability[train_partition,:])\n",
    "reachability_valid = np.copy(full_reachability[valid_partition,:])\n",
    "#Ground-truth labels:\n",
    "ground_truth_train = np.copy(full_ground_truth[train_partition])\n",
    "ground_truth_valid = np.copy(full_ground_truth[valid_partition])\n",
    "\n",
    "\n",
    "#Sanity checks... (ensure that all are selected, no overlap, etc.)\n",
    "if len(np.unique(train_partition)) != np.sum(N_per_class_train): sys.exit(\"Reps. in train partition\")\n",
    "if len(np.unique(valid_partition)) != np.sum(N_per_class_valid): sys.exit(\"Reps. in valid partition\")\n",
    "if len(np.unique(np.hstack((train_partition,valid_partition)))) != np.sum(N_per_class_train)+np.sum(N_per_class_valid):\n",
    "    sys.exit(\"Reps in the merge between train and valid partitions\")\n",
    "#Sanity checks: shape of reachability matrix\n",
    "if reachability_train.shape[0]     != np.sum(N_per_class_train): sys.exit(\"Wrong rows in train\")\n",
    "if reachability_valid.shape[0]     != np.sum(N_per_class_valid): sys.exit(\"Wrong rows in valid\")\n",
    "if reachability_train.shape[1]     != num_classes:                     sys.exit(\"Wrong cols in train\")\n",
    "if reachability_valid.shape[1]     != num_classes:                     sys.exit(\"Wrong cols in valid\")\n",
    "#Sanity checks: initial distributions\n",
    "if len(ground_truth_train)         != np.sum(N_per_class_train): sys.exit(\"Wrong length in train GT\")\n",
    "if len(ground_truth_valid)         != np.sum(N_per_class_valid): sys.exit(\"Wrong length in valid GT\")\n",
    "if np.any( [np.sum(ground_truth_train==l)!=N_per_class_train[l] for l in range(num_classes)] ):\n",
    "    sys.exit(\"Train sampling did not maintain the initial prob. distr.!\")\n",
    "if np.any( [np.sum(ground_truth_valid==l)!=N_per_class_valid[l] for l in range(num_classes)] ):\n",
    "    sys.exit(\"Valid sampling did not maintain the initial prob. distr.!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NUMBER OF SAMPLES TO CONSIDER DURING THE GENERATION OF TARGET DISTRIBUTIONS (TO ESTIMATE THE P-VALUE )\n",
    "eval_n_samp  = int(np.sum(N_per_class_valid)) #same size that we will encounter in validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample 'non-significantly different' target distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "tmp_n_dirichlets   = 1000 #number of distributions to sample\n",
    "p_y_obj_set_signif = np.zeros((tmp_n_dirichlets, len(labels)))\n",
    "p_y_obj_set        = np.zeros((tmp_n_dirichlets, len(labels)))\n",
    "\n",
    "#Sample the random Dirichlet distributions\n",
    "min_thr_freq = 50 #If a distribution has a frequency lower than this in any category, discard it\n",
    "\n",
    "for i_p in range(tmp_n_dirichlets):\n",
    "    cur_sampled_dist = np.copy(np.random.dirichlet([1 for i in range(len(labels))]))\n",
    "    while np.min(cur_sampled_dist*N_samp)<min_thr_freq:\n",
    "        cur_sampled_dist = np.copy(np.random.dirichlet([1 for i in range(len(labels))]))\n",
    "    p_y_obj_set_signif[i_p,:] = np.copy(cur_sampled_dist)\n",
    "    \n",
    "    \n",
    "#Adjust the sampled distributions so that they are not significantly different to the initial distribution    \n",
    "for i_p in range(p_y_obj_set_signif.shape[0]):\n",
    "    print(i_p)\n",
    "    p_y_target_signif = np.copy(p_y_obj_set_signif[i_p,:])\n",
    "    p_y_target = make_py_target_not_significant(p_y_init, p_y_target_signif,\n",
    "                                                py_test_tol, py_test_width,\n",
    "                                                eval_n_samp, labels, selected_stat_test)\n",
    "\n",
    "    p_y_obj_set[i_p,:] = np.copy(p_y_target) #Save the adjusted distribution\n",
    "    \n",
    "    #Sanity checks\n",
    "    assert evaluate_test(labels, eval_n_samp, p_y_init, p_y_target, selected_stat_test)>(py_test_tol+py_test_width)\n",
    "    assert np.round(np.min(p_y_target*N_samp))>=min_thr_freq, \"Not all the frequencies are above the threshold\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute our methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checkpoints to see whether during the process the statistical test detects significant label-shifts\n",
    "checkpoints = np.linspace(100, np.sum(N_per_class_valid), num=10, dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_per_class_train = np.copy(N_per_class_train) \n",
    "samples_per_class_valid = np.copy(N_per_class_valid)\n",
    "print(\"samples_per_class_train:\", samples_per_class_train)\n",
    "print(\"samples_per_class_valid:\", samples_per_class_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method to sample T\n",
    "get_row_probs_method = 1 #(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "#SUCCES AND FOOLING RATE\n",
    "success_optimization = np.zeros(len(p_y_obj_set), dtype=int) #succes generating T in the optimization\n",
    "fooling_rates        = np.zeros(len(p_y_obj_set)) #fooling rate of the attack for the individual samples\n",
    "#opt_fooling_rate     = np.zeros(len(p_y_obj_set)) #Opt FR that can be obtained in the current valid. set\n",
    "#KULLBACK-LEIBLER\n",
    "kl_diver_orig        = np.zeros(len(p_y_obj_set)) #kullback-leibler divergences between original & empirical \n",
    "kl_diver_obj         = np.zeros(len(p_y_obj_set)) #kullback-leibler divergence between objective & empirical\n",
    "kl_diver_orig_obj    = np.zeros(len(p_y_obj_set)) #kullback-leibler divergence between original & empirical\n",
    "#DISTANCE-BASED METRICS\n",
    "max_diff_orig        = np.zeros(len(p_y_obj_set)) #Max  diff. between original & empirical\n",
    "mean_diff_orig       = np.zeros(len(p_y_obj_set)) #Mean diff. between original & empirical\n",
    "max_diff_obj         = np.zeros(len(p_y_obj_set)) #Max  diff. between objective & empirical\n",
    "mean_diff_obj        = np.zeros(len(p_y_obj_set)) #Mean diff. between objective & empirical\n",
    "max_diff_orig_obj    = np.zeros(len(p_y_obj_set)) #Max  diff. between original & objective\n",
    "mean_diff_orig_obj   = np.zeros(len(p_y_obj_set)) #Mean diff. between original & objective\n",
    "#CORRELATIONS\n",
    "spearman_obj         = np.zeros(len(p_y_obj_set)) #Spearman correlation between objective & empirical\n",
    "pearson_obj          = np.zeros(len(p_y_obj_set)) #Pearson correlation between objective & empirical\n",
    "\n",
    "# ORIGINAL AND PREDICTED CLASSES\n",
    "save_orig_classes = np.zeros((len(p_y_obj_set), len(ground_truth_valid)), dtype=int)  # Ground-truth classes of the validation set\n",
    "save_adv_classes  = np.zeros((len(p_y_obj_set), len(ground_truth_valid)), dtype=int)  # Adversarial classes sampled for the validation set\n",
    "\n",
    "\n",
    "# PVALUES\n",
    "#During the validation process (intermediate steps)\n",
    "pval_orig_checks  = np.zeros((len(p_y_obj_set), len(checkpoints))) #Shift significance between original and \"clean\" empirical\n",
    "pval_adv_checks   = np.zeros((len(p_y_obj_set), len(checkpoints))) #Shift significance between original and \"adv.\"  empirical\n",
    "\n",
    "        \n",
    "#We also compute beforehand the maximum FR that can be obtained in the valid set.\n",
    "#We do it now because it only depends on the \"training\" set, not in the objective prob. distr.\n",
    "sum_reach_valid_rowise = np.sum(reachability_valid, axis=1) #Sum the number of classes that can be reached from each input\n",
    "#Count the ratio of cases in which more than one class can be reached (note that the source class can always\n",
    "#be reached)\n",
    "opt_fooling_rate = np.sum(sum_reach_valid_rowise>1)/float(reachability_valid.shape[0])\n",
    "\n",
    "\n",
    "#Sanity checks\n",
    "if len(sum_reach_valid_rowise)!=reachability_valid.shape[0]: sys.exit(\"Wrong axis for sum\")\n",
    "if len(sum_reach_valid_rowise)!=np.sum(N_per_class_valid):   sys.exit(\"Wrong axis for sum\")\n",
    "if np.any(sum_reach_valid_rowise<1):             sys.exit(\"Zero-row in reachability_valid\")\n",
    "if np.any(np.sum(reachability_train, axis=1)<1): sys.exit(\"Zero-row in reachability_train\")  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Compute the auxiliary matrix R \n",
    "R = np.zeros((num_classes,num_classes))\n",
    "for i in range(reachability_train.shape[0]):\n",
    "    cur_gt_label = ground_truth_train[i] #ground-truth label\n",
    "    for target_class in range(num_classes):\n",
    "        if reachability_train[i,target_class]==1: \n",
    "            R[cur_gt_label,target_class] += 1\n",
    "\n",
    "#Sanity checks:\n",
    "assert np.sum(R)==np.sum(reachability_train)\n",
    "for i in range(num_classes):\n",
    "    assert R[i,i] == samples_per_class_train[i]\n",
    "\n",
    "\n",
    "#Process every p_y_obj in the set\n",
    "##################################\n",
    "for i_p in range(len(p_y_obj_set)):\n",
    "    p_y_obj = p_y_obj_set[i_p]\n",
    "\n",
    "    #Generate the transition matrix (using Method 3: Element-wise Transformation Method - EWTM)\n",
    "    T_norm, solver_status = generate_T_method_3(p_y_init, samples_per_class_train, R, p_y_obj, num_classes) \n",
    "    \n",
    "    #Sanity checks\n",
    "    if solver_status!=\"Optimal\":\n",
    "        warnings.warn(\"Not solution found\")\n",
    "        continue\n",
    "\n",
    "    #Check that results are correct\n",
    "    T_norm = np.clip(T_norm, a_min=0.0, a_max=1.0) #clip to avoid precision errors\n",
    "    report_failure = check_results(T_norm, p_y_init, p_y_obj, num_classes, tolerance=1e-5) #check constraints\n",
    "    if report_failure: sys.exit(\"Failure reported in the generated T!\")\n",
    "\n",
    "\n",
    "    # Evaluate the effectiveness of the generated transition matrix in another set of inputs\n",
    "    original_props, \\\n",
    "    adversarial_props, \\\n",
    "    original_classes, \\\n",
    "    predicted_classes = validate_T_matrix(reachability_valid,\n",
    "                                          ground_truth_valid,\n",
    "                                          get_row_probs_method,\n",
    "                                          T_norm,  num_classes)\n",
    "\n",
    "    #SUCCESS AND FOOLING RATE\n",
    "    success_optimization[i_p] = 1  #Mark that a solution to the linear program was found (i.e., a matrix T)\n",
    "    fooling_rates[i_p] = np.sum(original_classes!=predicted_classes)/len(original_classes)\n",
    "\n",
    "    #DISTANCE-BASED METRICS\n",
    "    max_diff_orig[i_p]      = np.max( np.abs( np.array(original_props) - np.array(adversarial_props) ))\n",
    "    mean_diff_orig[i_p]     = np.mean(np.abs( np.array(original_props) - np.array(adversarial_props) ))\n",
    "    max_diff_obj[i_p]       = np.max( np.abs( np.array(p_y_obj)  - np.array(adversarial_props) ))\n",
    "    mean_diff_obj[i_p]      = np.mean(np.abs( np.array(p_y_obj)  - np.array(adversarial_props) ))\n",
    "    max_diff_orig_obj[i_p]  = np.max( np.abs( np.array(p_y_obj)  - np.array(original_props) ))\n",
    "    mean_diff_orig_obj[i_p] = np.mean(np.abs( np.array(p_y_obj)  - np.array(original_props) ))\n",
    "    #CORRELATIONS\n",
    "    spearman_obj[i_p] = sp.stats.spearmanr(adversarial_props,p_y_obj)[0]\n",
    "    pearson_obj[i_p]  = sp.stats.pearsonr(adversarial_props,p_y_obj)[0]\n",
    "    #KULLBACK-LEIBLER\n",
    "    kl_diver_orig[i_p]     = entropy(original_props, adversarial_props)\n",
    "    kl_diver_obj[i_p]      = entropy(p_y_obj,        adversarial_props)\n",
    "    kl_diver_orig_obj[i_p] = entropy(original_props, p_y_obj)\n",
    "    #Sanity check: avoid INF values of the KL because of zeros in the produced distribution\n",
    "    if np.isinf(kl_diver_orig[i_p]) or np.isinf(kl_diver_obj[i_p]):\n",
    "        if np.any(adversarial_props==0.0): \n",
    "            #Laplace smoothing\n",
    "            sm_adv_props = np.copy(adversarial_props)\n",
    "            sm_adv_props = np.array([sm_adv_props[i]*np.sum(N_per_class_valid)+1 for i in range(num_classes)])\n",
    "            sm_adv_props = np.array([sm_adv_props[i]/np.sum(sm_adv_props) for i in range(num_classes)])\n",
    "            if np.abs(np.sum(sm_adv_props)-1.0)>1e-8: sys.exit(\"Check Laplacian smoothing\")\n",
    "            kl_diver_orig[i_p] = entropy(original_props, sm_adv_props)\n",
    "            kl_diver_obj[i_p]  = entropy(p_y_obj,        sm_adv_props)\n",
    "            if np.isinf(kl_diver_orig[i_p]) or np.isinf(kl_diver_obj[i_p]): sys.exit(\"Check KL computation\")\n",
    "        else: sys.exit(\"Check KL computation (inf/nan but nonzeros)\")\n",
    "            \n",
    "    # ORIGINAL / ADVERSARIAL CLASSES\n",
    "    save_orig_classes[i_p,:] = np.copy(original_classes)\n",
    "    save_adv_classes[i_p, :] = np.copy(predicted_classes)\n",
    "\n",
    "\n",
    "    #Compute the \"intermediate\" p-values (i.e., during the attack process)\n",
    "    for i_check, n_check in enumerate(checkpoints):\n",
    "        #Select only the first \"n_check\" samples\n",
    "        samp_orig = np.copy(original_classes[0:n_check])\n",
    "        samp_pred = np.copy(predicted_classes[0:n_check])\n",
    "        assert len(samp_orig)==n_check and len(samp_pred)==n_check, \"Wrong sizes\" #Sanity check\n",
    "        #Get the corresponding proportions\n",
    "        samp_p_y_orig = np.array([np.sum(samp_orig==l) for l in range(len(labels))]) / n_check\n",
    "        samp_p_y_adv  = np.array([np.sum(samp_pred==l) for l in range(len(labels))]) / n_check\n",
    "        #Evaluate the statistical test\n",
    "        pval_orig_checks[i_p,i_check] = evaluate_test(labels, n_check, py_exp=p_y_init, py_obs=samp_p_y_orig, test_method=selected_stat_test)\n",
    "        pval_adv_checks[i_p, i_check] = evaluate_test(labels, n_check, py_exp=p_y_init, py_obs=samp_p_y_adv,  test_method=selected_stat_test)\n",
    "        #Sanity checks\n",
    "        assert np.abs(np.sum(samp_p_y_orig)-1.0)<1e-8 and np.abs(np.sum(samp_p_y_adv)-1.0)<1e-8\n",
    "        assert np.all(samp_p_y_orig>=0.0) and np.all(samp_p_y_orig<=1.0)\n",
    "        assert np.all(samp_p_y_adv >=0.0) and np.all(samp_p_y_adv <=1.0)\n",
    "\n",
    "\n",
    "\n",
    "#If a path was set for save_path, save the results\n",
    "if not (save_path is None):\n",
    "    #String to be appended at the end of the filenames, to identify the configuration\n",
    "    param_appendix = \"_eps_%s_init_%d.npy\" % (str(max_dist_thr), p_y_init_idx)\n",
    "    \n",
    "    # SAVE THE SUCCES INFORMATION AND FOOLING RATES\n",
    "    np.save(save_path + \"success_opt\"       + param_appendix, success_optimization)\n",
    "    np.save(save_path + \"fr\"                + param_appendix, fooling_rates)\n",
    "    np.save(save_path + \"opt_fr\"            + param_appendix, opt_fooling_rate)\n",
    "    # SAVE THE KULLBACK-LEIBLER INFORMATION\n",
    "    np.save(save_path + \"kl_orig\"           + param_appendix, kl_diver_orig)\n",
    "    np.save(save_path + \"kl_obj\"            + param_appendix, kl_diver_obj)\n",
    "    np.save(save_path + \"kl_orig_obj\"       + param_appendix, kl_diver_orig_obj)\n",
    "    # SAVE THE DISTANCE BASED METRICS\n",
    "    np.save(save_path + \"max_dif_orig\"      + param_appendix, max_diff_orig)\n",
    "    np.save(save_path + \"mean_dif_orig\"     + param_appendix, mean_diff_orig)\n",
    "    np.save(save_path + \"max_dif_obj\"       + param_appendix, max_diff_obj)\n",
    "    np.save(save_path + \"mean_dif_obj\"      + param_appendix, mean_diff_obj)\n",
    "    np.save(save_path + \"max_dif_orig_obj\"  + param_appendix, max_diff_orig_obj)\n",
    "    np.save(save_path + \"mean_dif_orig_obj\" + param_appendix, mean_diff_orig_obj)\n",
    "    # SAVE THE CORRELATIONS\n",
    "    np.save(save_path + \"spearman_obj\"      + param_appendix, spearman_obj)\n",
    "    np.save(save_path + \"pearson_obj\"       + param_appendix, pearson_obj)\n",
    "\n",
    "    # SAVE THE ORIGINAL / ADVERSARIAL CLASSES\n",
    "    np.save(save_path + \"save_orig_classes\" + param_appendix, save_orig_classes)\n",
    "    np.save(save_path + \"save_adv_classes\"  + param_appendix, save_adv_classes)\n",
    "\n",
    "    # SAVE THE PVALUES OF THE STATISTICAL TEST USED BY THE LABEL-SHIFT DETECTOR\n",
    "    np.save(save_path + \"pval_orig_checks\"  + param_appendix, pval_orig_checks)\n",
    "    np.save(save_path + \"pval_adv_checks\"   + param_appendix, pval_adv_checks)\n",
    "    np.save(save_path + \"checkpoints\"       + param_appendix, checkpoints)\n",
    "\n",
    "    # ASVE THE INITIAL AND SET OF TARGET DISTRIBUTIONS\n",
    "    np.save(save_path + \"p_y_init\"          + param_appendix, p_y_init)\n",
    "    np.save(save_path + \"p_y_obj_set\"       + param_appendix, p_y_obj_set)\n",
    "    \n",
    "    # SAVE THE REMAINING AUXILIARY INFORMATION\n",
    "    np.save(save_path + \"py_tolerance\"      + param_appendix, py_test_tol)\n",
    "    np.save(save_path + \"py_tol_width\"      + param_appendix, py_test_width)\n",
    "    print(\"Results saved\")\n",
    "    \n",
    "    \n",
    "print(\"Job done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
